- publication:
  title: "Can We Talk Models Into Seeing the World Differently?"
  authors: <b>Paul Gavrikov</b>, Jovita Lukasik, Steffen Jung, Robert Geirhos, Muhammad Jehanzeb Mirza, Margret Keuper, Janis Keuper
  paperlink: https://arxiv.org/abs/2403.09193
  conference: "ICLR, 2025 (initially at CVPR Workshops, 2024)"
  time: 2024
  highlight: True
  short: "We investigate the propagation of visual biases in LLM-powered Vision-Language Models (VLMs). Through the lense of the texture/shape bias, we find that not only does the multi-modal fusion with LLMs impact the inherited bias from a vision encoder, but this also allows us to simply prompt the model to change the bias, or said casually: we can talk the model into seeing the world differently."
  details: <a href="https://arxiv.org/abs/2403.09193">arXiv</a> / <a href="https://openreview.net/forum?id=iVMcYxTiVM">paper</a> / <a href="https://github.com/paulgavrikov/vlm_shapebias">code</a>
  image: images/tn_vlm_shapebias.png
- publication:
  title: "How Do Training Methods Influence the Utilization of Vision Models?"
  authors: <b>Paul Gavrikov</b>, Shashank Agnihotri, Margret Keuper, Janis Keuper
  paperlink: https://arxiv.org/abs/2410.14470
  conference: "NeurIPS Workshops, 2024"
  time: 2024
  highlight: True
  short: "In this preliminary study, we analyze the influence of training methods on the utilization of layers in ImageNet classification models while keeping training data and the architecture fixed."
  details: <a href="https://arxiv.org/abs/2410.14470">arXiv</a> / <a href="https://github.com/paulgavrikov/layer_criticality">code</a>
  image: images/tn_layer_crit.png
- publication:
  title: "GLOV: Guided Large Language Models as Implicit Optimizers for Vision Language Models"
  authors: M. Jehanzeb Mirza, Mengjie Zhao, Zhuoyuan Mao, Sivan Doveh, Wei Lin, <b>Paul Gavrikov</b>, Michael Dorkenwald, Shiqi Yang, Saurav Jha, Hiromi Wakaki, Yuki Mitsufuji, Horst Possegger, Rogerio Feris, Leonid Karlinsky, James Glass
  paperlink: https://arxiv.org/abs/2410.06154
  conference: "Preprint"
  time: 2024
  highlight: False
  short: "We introduce GLOV, a method that enables LLMs to optimize VLMs by generating and refining prompts for downstream vision tasks, achieving significant performance improvements across various datasets."
  details: <a href="https://arxiv.org/abs/2410.06154">arXiv</a> / <a href="https://github.com/jmiemirza/GLOV">code</a>
  image: images/tn_glov.png
- publication:
  title: "Can Biases in ImageNet Models Explain Generalization?"
  authors: <b>Paul Gavrikov</b>, Janis Keuper
  paperlink: https://arxiv.org/abs/2404.01509
  conference: "CVPR, 2024"
  time: 2024
  highlight: True
  short: "We investigate the generalization capabilities of neural networks from the perspective of shape bias, spectral biases, and the critical band. Our results show that even when we fix the architecture these indicators are not reliable predictors of generalization performance."
  details: <a href="https://openaccess.thecvf.com/content/CVPR2024/html/Gavrikov_Can_Biases_in_ImageNet_Models_Explain_Generalization_CVPR_2024_paper.html">paper</a> / <a href="http://arxiv.org/abs/2404.01509">arXiv</a> / <a href="https://github.com/paulgavrikov/biases_vs_generalization">code</a>
  image: images/small/tn_bias_v_generalization.jpg
- publication:
  title: "Improving Native CNN Robustness with Filter Frequency Regularization"
  authors: Jovita Lukasik*, <b>Paul Gavrikov*</b>, Janis Keuper, Margret Keuper
  paperlink: https://openreview.net/forum?id=2wecNCpZ7Y
  time: 2023
  highlight: True
  short: "We propose controlling the frequency content of learned convolution filters in vision CNNs. This results in model that are natively more robust to adversarial robustness and corruptions, generalize better, and are generally more aligned with human vision."
  conference: "TMLR, 2023"
  details: <a href="https://openreview.net/forum?id=2wecNCpZ7Y">paper</a> / <a href="https://github.com/jovitalukasik/filter_freq_reg">code</a> #<!--/ <a href="">arXiv</a> -->
  image: images/small/tn_filterfreq.png
- publication:
  title: "Don't Look into the Sun: Adversarial Solarization Attacks on Image Classifiers"
  paperlink: https://arxiv.org/abs/2308.12661
  authors: <b>Paul Gavrikov</b>, Janis Keuper
  time: 2023
  short: "We present a new adversarial attack based on image solarization. Despite being conceptually simple, the attack is effective, cheap to compute, and does not risk destroying the global structure of natural images. It also serves as a universal black-box attack against models trained with the legacy ImageNet training recipe."
  conference: "Preprint"
  details: <a href="http://arxiv.org/abs/2308.12661">arXiv</a> / <a href="https://github.com/paulgavrikov/adversarial_solarization">code</a>
  image: images/small/tn_advsol.png
- publication:
  title: "On the Interplay of Convolutional Padding and Adversarial Robustness"
  authors: <b>Paul Gavrikov</b>, Janis Keuper
  paperlink: https://openaccess.thecvf.com/content/ICCV2023W/BRAVO/html/Gavrikov_On_the_Interplay_of_Convolutional_Padding_and_Adversarial_Robustness_ICCVW_2023_paper.html
  time: 2023
  short: "Our study examines the relationship between padding in Convolutional Neural Networks (CNNs) and vulnerabilities to adversarial attacks. We show that adversarial attacks result in different perturbation anomalies at image boundaries depending on the padding mode and discuss which mode is the best for adversarial settings. (Spoiler: it's zero padding)"
  conference: "ICCV Workshops, 2023"
  details: <a href="https://openaccess.thecvf.com/content/ICCV2023W/BRAVO/html/Gavrikov_On_the_Interplay_of_Convolutional_Padding_and_Adversarial_Robustness_ICCVW_2023_paper.html">paper</a> / <a href="http://arxiv.org/abs/2308.06612">arXiv</a>
  image: images/small/tn_atpadding.png
- publication:
  title: "An Extended Study of Human-like Behavior under Adversarial Training"
  authors: <b>Paul Gavrikov</b>, Janis Keuper, Margret Keuper
  paperlink: https://openaccess.thecvf.com/content/CVPR2023W/AML/html/Gavrikov_An_Extended_Study_of_Human-Like_Behavior_Under_Adversarial_Training_CVPRW_2023_paper.html
  time: 2023
  short: "Our analysis reveals how different forms of adversarial training (AT) affect human-like behavior of CNNs and Transformers. Additionally, we propose a hypothesis of why AT increases shape bias and in which scenarios it can improve out-of-distribution generalization from a frequency perspective."
  conference: "CVPR Workshops, 2023"
  details: <a href="https://openaccess.thecvf.com/content/CVPR2023W/AML/html/Gavrikov_An_Extended_Study_of_Human-Like_Behavior_Under_Adversarial_Training_CVPRW_2023_paper.html">paper</a> / <a href="https://github.com/paulgavrikov/adversarial_training_vs_humans">code</a> / <a href="https://arxiv.org/abs/2303.12669">arXiv</a>
  image: images/small/tn_atshapebias.png
- publication:
  title: "The Power of Linear Combinations: Learning with Random Convolutions"
  authors: <b>Paul Gavrikov</b>, Janis Keuper
  paperlink: https://arxiv.org/abs/2301.11360
  time: 2023
  highlight: True
  short: "We question if learning spatial convolution filters is necessary. Even with default i.i.d. random inits, we can achieve 75.66% validation acc with ResNet-50 on ImageNet without ever learning any spatial convolution weight. Additionally, random filters can be more robust against adversarial attacks than learned filters."
  conference: "In Review"
  details: <a href="https://arxiv.org/abs/2301.11360">arXiv</a>
  image: images/small/tn_1x1_conv.png
- publication:
  title: "Does Medical Imaging learn different Convolution Filters?"
  authors: <b>Paul Gavrikov</b>, Janis Keuper
  paperlink: https://arxiv.org/abs/2210.13799
  time: 2022
  short: "Earlier, we showed that, on average, convolution filters only show minor drifts when comparing various dimensions, including the learned task, image domain, or dataset. However, medical imaging models showed significant outliers through spiky filter distributions. We revisit this observation and perform an in-depth analysis of medical imaging models."
  conference: "NeurIPS Workshops, 2022"
  details: <a href="https://arxiv.org/abs/2210.13799">arXiv</a>
  image: images/small/tn_medicalfilters.png
- publication:
  title: "Robust Models are less Over-Confident"
  authors: Julia Grabinski, <b>Paul Gavrikov</b>, Janis Keuper, Margret Keuper
  paperlink: https://proceedings.neurips.cc/paper/2022/hash/ff887781480973bd3cb6026feb378d1e-Abstract-Conference.html
  time: 2022
  highlight: True
  short: "Adversarial Training (AT) leads to models that are significantly less overconfident with their decisions, even on clean data, than non-robust models. The analysis shows that not only AT, but also the models' building blocks (like activation functions and pooling) have a strong influence on the models' prediction confidences."
  conference: "NeurIPS, 2022 (initially at ICML Workshops, 2022)"
  details: <a href="https://proceedings.neurips.cc/paper_files/paper/2022/hash/ff887781480973bd3cb6026feb378d1e-Abstract-Conference.html">paper</a> / <a href="https://arxiv.org/abs/2210.05938">arXiv</a> / <a href="https://github.com/GeJulia/robustness_confidences_evaluation">code</a>
  image: images/small/tn_robustconf.png
- publication:
  title: "Adversarial Robustness through the Lens of Convolutional Filters"
  authors: <b>Paul Gavrikov</b>, Janis Keuper
  paperlink: https://openaccess.thecvf.com/content/CVPR2022W/ArtOfRobust/html/Gavrikov_Adversarial_Robustness_Through_the_Lens_of_Convolutional_Filters_CVPRW_2022_paper.html
  conference: "CVPR Workshops, 2022"
  short: "We investigate 3x3 convolution filters that form in adversarially-trained robust models and find that these models form more diverse, less sparse, and more orthogonal filters than their normal counterparts. The largest differences are found in the deepest layers and the very first convolution layer which forms highly distinct thresholding filters."
  time: 2022
  details: <a href="https://openaccess.thecvf.com/content/CVPR2022W/ArtOfRobust/html/Gavrikov_Adversarial_Robustness_Through_the_Lens_of_Convolutional_Filters_CVPRW_2022_paper.html">paper</a> / <a href="https://arxiv.org/abs/2204.02481">arXiv</a> / <a href="https://github.com/paulgavrikov/cvpr22w_RobustnessThroughTheLens">code</a>
  image: images/small/tn_advfilter.png
- publication:
  title: "CNN Filter DB: An Empirical Investigation of Trained Convolutional Filters"
  type: <font color="red"><strong>Oral Presentation</strong></font>
  authors: <b>Paul Gavrikov</b>, Janis Keuper
  paperlink: https://openaccess.thecvf.com/content/CVPR2022/html/Gavrikov_CNN_Filter_DB_An_Empirical_Investigation_of_Trained_Convolutional_Filters_CVPR_2022_paper.html
  conference: "CVPR, 2022"
  highlight: True
  short: "We collected and publicly provided a dataset with over 1.4 billion 3x3 convolution filters from hundreds of trained CNNs. Our observations show that - surprisingly - models learn highly similar filter pattern distributions independent of task and dataset, but differ by model architecture. We also propose methods to measure the quality of filters to detect overparameterization or underfitting and show that many publicly available models suffer from \"degenerated\" filters."
  time: 2022
  details: <a href="https://openaccess.thecvf.com/content/CVPR2022/html/Gavrikov_CNN_Filter_DB_An_Empirical_Investigation_of_Trained_Convolutional_Filters_CVPR_2022_paper.html">paper</a> / <a href="https://arxiv.org/abs/2203.15331">arXiv</a> / <a href="https://github.com/paulgavrikov/CNN-Filter-DB/">code</a>
  image: images/small/tn_cnnfilterdb.png
- publication:
  title: "An Empirical Investigation of Model-to-Model Distribution Shifts in Trained Convolutional Filters"
  authors: <b>Paul Gavrikov</b>, Janis Keuper
  paperlink: https://openreview.net/forum?id=2st0AzxC3mh
  conference: "NeurIPS Workshops, 2021"
  time: 2021
  short: "This paper looks at distribution shifts in filter weights used for various computer vision tasks. We collected data from hundreds of trained CNNs and analyzed the distribution shifts along different axes of meta-parameters. We found interesting distribution shifts between trained filters, and argue that this is a valuable source for further investigation into understanding the impact of shifts in the input data on the generalization abilities of CNN models."
  details: <a href="https://openreview.net/forum?id=2st0AzxC3mh">paper</a> / <a href="https://arxiv.org/abs/2201.08465">arXiv</a> / <a href="https://github.com/paulgavrikov/CNN-Filter-DB/">code</a>
  image: images/small/tn_empirical.png
- publication:
  title: "A Low Power and Low Latency Scan Algorithm for Bluetooth Low Energy Radios with Energy Detection Mechanisms"
  type: <font color="red"><strong>Best Paper Award</strong></font>
  authors: <b>Paul Gavrikov</b>, Matthias Lai, Thomas Wendt
  paperlink: https://ieeexplore.ieee.org/document/8964210
  conference: "APWiMob, 2019"
  short: "A new, more efficient algorithm for Bluetooth scanning is presented that uses less power and can scale with incoming network traffic. The algorithm does not require any changes to advertisers, so it is compatible with existing devices, and performance evaluation shows that it is more efficient than existing methods."
  time: 2019
  details: <a href="https://ieeexplore.ieee.org/document/8964210">paper</a>
  image: images/small/tn_ble_scan.png
- publication:
  title: "Exploring non-idealities in real device implementations of Bluetooth Mesh"
  authors: <b>Paul Gavrikov</b>, Matthias Lai, Thomas Wendt
  paperlink: https://www.igi-global.com/article/exploring-non-idealities-in-real-device-implementations-of-bluetooth-mesh/265148
  conference: "Wireless Telecommunications Symposium 2019 (presentation) and IJITN"
  time: 2019
  short: "We compare the performance of Bluetooth Mesh implementations on real chipsets against the ideal implementation of the specification. It is shown that there are non-idealities in the underlying Bluetooth Low Energy specification in real chipsets and in the implementation of Mesh, which introduces an unruly transmission as well as reception behavior. These effects lead to an impact on transmission rate, reception rate, latency, as well as a more significant impact on the average power consumption."
  details: <a href="https://www.igi-global.com/article/exploring-non-idealities-in-real-device-implementations-of-bluetooth-mesh/265148">paper</a>
  image: images/small/tn_ble_nonidealities.png
- publication:
  title: "Using Bluetooth Low Energy to trigger a robust ultra-low power FSK wake-up receiver"
  authors: <b>Paul Gavrikov</b>, Pascal E. Verboket, Tolgay Ungan, Markus Müller, Matthias Lai, Christian Schindelhauer, Leonhard M. Reindl, Thomas Wendt
  paperlink: https://ieeexplore.ieee.org/document/8618031
  conference: ICECS, 2018
  time: 2018
  short: "We discuss a new approach to using BLE packets to create an FSK-like addressable wake-up packet. A wake-up receiver system was developed from off-the-shelf components to detect these packets. This system is more robust than traditional OOK wake-up systems and has a sensitivity of -47.8 dBm at a power consumption of 18.5 uW during passive listening. The system has a latency of 31.8 ms with a symbol rate of 1437 Baud."
  details: <a href="https://ieeexplore.ieee.org/document/8618031">paper</a>
  image: images/small/tn_wur.png
